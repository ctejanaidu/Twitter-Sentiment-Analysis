{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Twitter Sentiment Analysis LSTM",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ctejanaidu/Twitter-Sentiment-Analysis/blob/main/Twitter_Sentiment_Analysis_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'twitter-airline-sentiment:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F17%2F742210%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240808%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240808T223129Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0becfcc1de3517e2b4292b2b21260c386067b0c5c06f974d470784c24dcba552a9853edfb3d195e993574fa0605708d8f172b7918ac8d1b96d2bf468d0c939deb8456f8ffd3e275a2fca56e06519e3cd71c9320d620a65dc4ea6527470e01d2b280fbc6850f8e1d2be630ab266f17853e0e0776ee64f7abb376ecf9e2bd016c2759eaaa17d9256ba15734b922e550473a47fd804dc6dcd230d3dedd187fb26662a1dcbc8e58d84a5d5e8cb81e62bd75b09e8735e68bae5b572ac9d7d87be1e9c2fb3dc372e4e0522d1dbaa7556e70cb312cb9fe56e07226a7f9e97a7fe739234c81acd5405d514afb46e124b9b346d566814d6ce96c2d2424c6a6d1d477566dc,twitter-and-reddit-sentimental-analysis-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F429085%2F815876%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240808%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240808T223129Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D51cd3a223a36d2ada269ad47c3bfcb6218a01da2d8d140da470c580dfa553257af748bbb7995d7ac1088cf29844f5591871b80ae39cd3205d36275b37d7a7a8b9568e69acaf96e72b105758c8ae40f1216fa41b5f07468c48c66b6e1f626f6c2e512a0133a7c131121a9ee6dfe5c579b87dfda7341c8b8234285c1ea2594bd915e7991d27b0bce2ba0daae273da14fd9575a0619a47604d479993d6cf1d1b55c09844627fa3467ce4e430abed5bcab9e99541249e89b80f29b435b092742dbf584bb2409c5bd83cf2d6850c3e8f25440181537903d35401cae62c6f71f7e39563f62e91d2377a9ebab61233ece56790ac0c11bae76d64bdbcec03facf6183c85,appletwittersentimenttexts:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F652925%2F1154930%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240808%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240808T223129Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D21ae0c2a2ac0731c4f0510c068ebde9076d30a88756c288078a69c08a834c8d7845ce5f65d7fc657f283870045d315894af0c58b6af14f6ccc0442c8ec801d4713d2361402b65c390e98935eb93aba95efe87aab16ed543bec1e67909418edb7b3256c5914ac7f5a18797a8a0210fe2da885c4a637c2d3e07a39d3bf08a2e860622bc9fc9d391fd2f533880ea31da51ef75d6200f9dfdd20e7483b21ce41c4a0a56b6ad80cb19172fc28862207afdfacda481c56afe0292959f4bff120b2718664b2901ee05402ed776750ebff84cfd19cf466f051fad17b73b6749c54c26542c5f2817c8e128ed98b378b4406768a122812024eac4885d9190ecaaa056794a1,sentiment-analysis-for-financial-news:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F622510%2F1192499%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240808%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240808T223129Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6033c99116a7041af2d426da0de3b52b5f5b7bb4c35d19dbef290a9789ef9ecad0c1a07c9190f330833d0cd0fc29ac2cdaaaacce46732751f6cd41fb91fcd4bd443e2918034d3e27be95c61948599043afdcf415ec8da0216d6f8e7d05bfb8b63385a8ecfdbdd7a55fec2a6a9fa690bd975019a9866bf29597e21674ab2f54d2061ac95601969737d5cf13bcee291d13925cf7b0c2c48a9082cb5ab5d3f3d049b715f9682e429af40c1384d65cfbf28f7af18e4bf211cb1057e89bbdf88522dff1b1433f7b0213c7e85990c42e900a9f3c5f268739d3107ce7ab50aa2ad9a05e7bc4df68642d4a52255a53757db887420b6825a2502b049725bfa1b8bbbacf9d,twitterdata:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F739333%2F1281216%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240808%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240808T223129Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D49c27c18e2bd8b2ab48e6d26660aecfc7dfc92caa4227cbcd650955daa9d902832a4fb3e3ddafb8553d805d4b2f7c4ed50194ad5786c72440a2b3e28aabf81a222b8a1a89721795086f0c5bb62c0f2a7a02b30104c86e2a83f2c79d16c359c943cc65e2d0947d6bbd13914cdc63cd30cbe05d1f0bd674f16767bfb12320711c46fe6668fc9ac0f64ab18b31e4da977d1b904caf1e250fa9153ce448f984b2265883dfe666ed23b913805a13239b15b160e8c5060877c7ca5dfc368990aafff8cc1741b56f541adf670efbe42c7bd0c33d401d5bf8f811ae33baa9c5e67c4f5e1c7dafa99974bab133411b4e4b6dbd693a529e683b44a2fff0f03fa08b7bf5588'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "CG8Ch1dZHlg5"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "This script performs EDA and then preprocesses multiple datasets to train a bidirectional LSTM model which is in turn used to predict the sentiments behind tweets fetched in real time using `tweepy` and classify them as positive negative or neutral.\n",
        "\n",
        "The model is then integrated with streamlit and deployed as a web-app.\n",
        "\n",
        "**Checkout the web-app:** [Sententia](https://share.streamlit.io/kritanjalijain/twitter_sentiment_analysis/main/app.py)"
      ],
      "metadata": {
        "_uuid": "7d741102-6897-4ac3-82ec-01e155f4ebed",
        "_cell_guid": "4fd1cc5c-3d33-4178-a761-ef5dfba36d96",
        "trusted": true,
        "id": "UQBpU7LdHlg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing and importing dependencies"
      ],
      "metadata": {
        "_uuid": "e323ac66-f138-4c99-84e7-ffc2d736df95",
        "_cell_guid": "e9af0762-88ad-444a-98d6-b60ff39fb1f7",
        "trusted": true,
        "id": "c7IWUgyTHlg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fetch tweets from twitter, we need to install the tweepy library. We will be using this package to pull tweets on which our model will make predictions."
      ],
      "metadata": {
        "_uuid": "f5296f63-a4d8-448f-8a66-4a5a74844b73",
        "_cell_guid": "ad397e61-b81d-48bc-bb0c-471762f5620e",
        "trusted": true,
        "id": "17qmFK-PHlg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweepy"
      ],
      "metadata": {
        "_uuid": "10589cdb-f83f-4854-ab71-52fba77151f2",
        "_cell_guid": "d5917246-e88f-4459-bfe1-0782b019a9aa",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:48:07.834952Z",
          "iopub.execute_input": "2021-06-15T13:48:07.835345Z",
          "iopub.status.idle": "2021-06-15T13:48:15.337758Z",
          "shell.execute_reply.started": "2021-06-15T13:48:07.835265Z",
          "shell.execute_reply": "2021-06-15T13:48:15.336845Z"
        },
        "trusted": true,
        "id": "jp0Sjm38Hlg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing\n",
        "import os\n",
        "import tweepy as tw #for accessing Twitter API\n",
        "\n",
        "\n",
        "#For Preprocessing\n",
        "import re    # RegEx for removing non-letter characters\n",
        "import nltk  #natural language processing\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "# For Building the model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "\n",
        "#For data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "%matplotlib inline\n",
        "\n",
        "pd.options.plotting.backend = \"plotly\""
      ],
      "metadata": {
        "_uuid": "c0e8bf21-9fc1-464b-a11b-6bdf4b0f9fce",
        "_cell_guid": "76f1f3d0-b6fe-44a9-993e-ecfe7f108944",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:48:15.341787Z",
          "iopub.execute_input": "2021-06-15T13:48:15.342066Z",
          "iopub.status.idle": "2021-06-15T13:48:22.030557Z",
          "shell.execute_reply.started": "2021-06-15T13:48:15.342038Z",
          "shell.execute_reply": "2021-06-15T13:48:22.028518Z"
        },
        "trusted": true,
        "id": "49d9tuyOHlg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "_uuid": "e68b1162-2764-4e03-9b85-b4786bb3c20a",
        "_cell_guid": "782f9485-6c3a-417a-bcd3-d8cdbb354969",
        "trusted": true,
        "id": "wK9ImZg-Hlg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning and prepping dataset"
      ],
      "metadata": {
        "_uuid": "88e14970-da9b-4403-af5e-c631eede9c19",
        "_cell_guid": "49547739-db4d-4559-b6d2-600ecd225d3e",
        "trusted": true,
        "id": "773aWseBHlg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Tweet dataset\n",
        "df1 = pd.read_csv('../input/twitter-and-reddit-sentimental-analysis-dataset/Twitter_Data.csv')\n",
        "# Output first five rows\n",
        "df1.head()"
      ],
      "metadata": {
        "_uuid": "a89e4673-96aa-4b2f-916c-6c6bce257519",
        "_cell_guid": "debf7898-99e6-442b-8102-b8414809d8e3",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:48:22.032452Z",
          "iopub.execute_input": "2021-06-15T13:48:22.033097Z",
          "iopub.status.idle": "2021-06-15T13:48:22.675333Z",
          "shell.execute_reply.started": "2021-06-15T13:48:22.033055Z",
          "shell.execute_reply": "2021-06-15T13:48:22.673006Z"
        },
        "trusted": true,
        "id": "7ELJNGG4Hlg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Tweet dataset\n",
        "df2 = pd.read_csv('../input/appletwittersentimenttexts/apple-twitter-sentiment-texts.csv')\n",
        "df2 = df2.rename(columns={'text': 'clean_text', 'sentiment':'category'})\n",
        "df2['category'] = df2['category'].map({-1: -1.0, 0: 0.0, 1:1.0})\n",
        "# Output first five rows\n",
        "\n",
        "df2.head()"
      ],
      "metadata": {
        "_uuid": "6b2e7439-e264-47de-a24e-f26299e6662f",
        "_cell_guid": "0a8f7e68-a86c-4185-aa69-041e2dbfbbcd",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:48:22.679457Z",
          "iopub.execute_input": "2021-06-15T13:48:22.681776Z",
          "iopub.status.idle": "2021-06-15T13:48:22.718052Z",
          "shell.execute_reply.started": "2021-06-15T13:48:22.681734Z",
          "shell.execute_reply": "2021-06-15T13:48:22.717213Z"
        },
        "trusted": true,
        "id": "xWyaHg7IHlg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `category` column has 3 values:\n",
        "1. 0 Indicating it is a Neutral Sentiment\n",
        "2. 1 Indicating a Postive Sentiment\n",
        "3. -1 Indicating a Negative Sentiment"
      ],
      "metadata": {
        "_uuid": "35ba0524-cb6d-4458-8d79-e27f52d9d2d9",
        "_cell_guid": "6d806168-0fd8-4c53-951f-ae424c6b18b5",
        "trusted": true,
        "id": "IAW7QS69Hlg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Tweet dataset\n",
        "df3 = pd.read_csv('../input/twitterdata/finalSentimentdata2.csv')\n",
        "df3 = df3.rename(columns={'text': 'clean_text', 'sentiment':'category'})\n",
        "df3['category'] = df3['category'].map({'sad': -1.0, 'anger': -1.0, 'fear': -1.0, 'joy':1.0})\n",
        "df3 = df3.drop(['Unnamed: 0'], axis=1)\n",
        "# Output first five rows\n",
        "df3.head()"
      ],
      "metadata": {
        "_uuid": "2438c526-1edc-4d76-be63-0e56a379a254",
        "_cell_guid": "0aadd60d-21c4-44c4-8fc9-4a5abb5ffcec",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:48:22.723596Z",
          "iopub.execute_input": "2021-06-15T13:48:22.725742Z",
          "iopub.status.idle": "2021-06-15T13:48:22.77159Z",
          "shell.execute_reply.started": "2021-06-15T13:48:22.72569Z",
          "shell.execute_reply": "2021-06-15T13:48:22.770871Z"
        },
        "trusted": true,
        "id": "J9ctjZvTHlg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Tweet dataset\n",
        "df4 = pd.read_csv('../input/twitter-airline-sentiment/Tweets.csv')\n",
        "df4 = df4.rename(columns={'text': 'clean_text', 'airline_sentiment':'category'})\n",
        "df4['category'] = df4['category'].map({'negative': -1.0, 'neutral': 0.0, 'positive':1.0})\n",
        "df4 = df4[['category','clean_text']]\n",
        "# Output first five rows\n",
        "df4.head()"
      ],
      "metadata": {
        "_uuid": "f7fb215b-48b9-49fc-a6d7-a54610aeddee",
        "_cell_guid": "ed299507-c682-4bfd-b997-146ab3552b77",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:48:22.777353Z",
          "iopub.execute_input": "2021-06-15T13:48:22.779441Z",
          "iopub.status.idle": "2021-06-15T13:48:22.921452Z",
          "shell.execute_reply.started": "2021-06-15T13:48:22.779404Z",
          "shell.execute_reply": "2021-06-15T13:48:22.92064Z"
        },
        "trusted": true,
        "id": "ldGaaQ2AHlg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df1, df2, df3, df4], ignore_index=True)"
      ],
      "metadata": {
        "_uuid": "40162102-3d8c-40cb-90aa-4768c908436e",
        "_cell_guid": "0725a44b-49c3-4bbc-b763-537bc252ffa9",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:48:22.926099Z",
          "iopub.execute_input": "2021-06-15T13:48:22.929072Z",
          "iopub.status.idle": "2021-06-15T13:48:22.939973Z",
          "shell.execute_reply.started": "2021-06-15T13:48:22.929001Z",
          "shell.execute_reply": "2021-06-15T13:48:22.93911Z"
        },
        "trusted": true,
        "id": "v_9XA6HLHlg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing data\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "_uuid": "201a6249-c24c-4d39-b438-3d1a763a604d",
        "_cell_guid": "486ed74c-3923-449d-9ec5-99f7dcd66087",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:48:22.941332Z",
          "iopub.execute_input": "2021-06-15T13:48:22.941741Z",
          "iopub.status.idle": "2021-06-15T13:48:22.971665Z",
          "shell.execute_reply.started": "2021-06-15T13:48:22.941707Z",
          "shell.execute_reply": "2021-06-15T13:48:22.970968Z"
        },
        "trusted": true,
        "id": "L17C1xxaHlg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop missing rows\n",
        "df.dropna(axis=0, inplace=True)"
      ],
      "metadata": {
        "_uuid": "4e2799f7-7cc8-457c-a43f-2ddfa67fabab",
        "_cell_guid": "06510275-87ba-47d5-9589-e78270258c10",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:48:22.974537Z",
          "iopub.execute_input": "2021-06-15T13:48:22.974836Z",
          "iopub.status.idle": "2021-06-15T13:48:23.02642Z",
          "shell.execute_reply.started": "2021-06-15T13:48:22.97481Z",
          "shell.execute_reply": "2021-06-15T13:48:23.025651Z"
        },
        "trusted": true,
        "id": "xPafYheRHlg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dimensionality of the data\n",
        "df.shape"
      ],
      "metadata": {
        "_uuid": "c90a041b-eb65-4a4b-805a-7a9900e8df02",
        "_cell_guid": "177ff162-f381-44ba-bb85-36ea40de02d1",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:48:23.028447Z",
          "iopub.execute_input": "2021-06-15T13:48:23.028691Z",
          "iopub.status.idle": "2021-06-15T13:48:23.036914Z",
          "shell.execute_reply.started": "2021-06-15T13:48:23.028668Z",
          "shell.execute_reply": "2021-06-15T13:48:23.035895Z"
        },
        "trusted": true,
        "id": "y56tW2BgHlg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map tweet categories\n",
        "df['category'] = df['category'].map({-1.0:'Negative', 0.0:'Neutral', 1.0:'Positive'})\n",
        "\n",
        "# Output first five rows\n",
        "df.head()"
      ],
      "metadata": {
        "_uuid": "163626db-91d9-484d-9024-dfdc7712be96",
        "_cell_guid": "ef27f6c0-14f1-4938-88c7-dce16525c2f3",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:48:23.038144Z",
          "iopub.execute_input": "2021-06-15T13:48:23.038684Z",
          "iopub.status.idle": "2021-06-15T13:48:23.059606Z",
          "shell.execute_reply.started": "2021-06-15T13:48:23.038647Z",
          "shell.execute_reply": "2021-06-15T13:48:23.058775Z"
        },
        "trusted": true,
        "id": "SCRD1bRDHlg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "_uuid": "25451ff0-d739-4029-9b13-447a5fa752db",
        "_cell_guid": "c7cde934-0ba3-4f17-add8-82cda374a45e",
        "trusted": true,
        "id": "9uHGk97qHlg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Visualisation-"
      ],
      "metadata": {
        "_uuid": "20ef8a9b-b557-4877-98ec-cf8b0f485703",
        "_cell_guid": "269af7d5-a4d8-4f40-871d-49928d690942",
        "trusted": true,
        "id": "jUwbrrdeHlg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The distribution of sentiments\n",
        "df.groupby('category').count().plot(kind='bar')"
      ],
      "metadata": {
        "_uuid": "92567094-5bd4-461b-89de-05bc3098138d",
        "_cell_guid": "93d87248-3293-4306-b672-6a3d1e551706",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:48:23.061019Z",
          "iopub.execute_input": "2021-06-15T13:48:23.061453Z",
          "iopub.status.idle": "2021-06-15T13:48:26.764979Z",
          "shell.execute_reply.started": "2021-06-15T13:48:23.061417Z",
          "shell.execute_reply": "2021-06-15T13:48:26.764133Z"
        },
        "trusted": true,
        "id": "jnqEDiivHlg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the distribution of tweet lengths"
      ],
      "metadata": {
        "_uuid": "c487d37b-8554-4bfd-bece-f87b2be4c103",
        "_cell_guid": "b9f1cd1a-3fcf-4a4a-8abe-01683e8850b7",
        "trusted": true,
        "id": "CaokmQ_VHlg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate tweet lengths\n",
        "tweet_len = pd.Series([len(tweet.split()) for tweet in df['clean_text']])\n",
        "\n",
        "# The distribution of tweet text lengths\n",
        "tweet_len.plot(kind='box')"
      ],
      "metadata": {
        "_uuid": "900abce1-10f0-4a3b-b3db-69bc92d90817",
        "_cell_guid": "c6d91b8e-7009-4d7b-9fb4-0ea0e8260bfb",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:48:26.766349Z",
          "iopub.execute_input": "2021-06-15T13:48:26.766712Z",
          "iopub.status.idle": "2021-06-15T13:48:28.770917Z",
          "shell.execute_reply.started": "2021-06-15T13:48:26.766674Z",
          "shell.execute_reply": "2021-06-15T13:48:28.769984Z"
        },
        "trusted": true,
        "id": "tlfqUQ5_Hlg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the distribution of text length for positive sentiment tweets"
      ],
      "metadata": {
        "_uuid": "dafbbd71-8135-441b-94a5-41045bc850de",
        "_cell_guid": "40b0cbba-a724-4b55-a306-60cef61614b7",
        "trusted": true,
        "id": "mc7dqA_ZHlg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(14,7))\n",
        "df['length'] = df.clean_text.str.split().apply(len)\n",
        "ax1 = fig.add_subplot(122)\n",
        "sns.histplot(df[df['category']=='Positive']['length'], ax=ax1,color='green')\n",
        "describe = df.length[df.category=='Positive'].describe().to_frame().round(2)\n",
        "\n",
        "ax2 = fig.add_subplot(121)\n",
        "ax2.axis('off')\n",
        "font_size = 14\n",
        "bbox = [0, 0, 1, 1]\n",
        "table = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\n",
        "table.set_fontsize(font_size)\n",
        "fig.suptitle('Distribution of text length for positive sentiment tweets.', fontsize=16)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "_uuid": "4a48ea39-76fb-48da-b8c0-3480809fecd1",
        "_cell_guid": "8b560255-1061-4a99-a6c8-e79b6f8e7d0a",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:48:28.772072Z",
          "iopub.execute_input": "2021-06-15T13:48:28.772392Z",
          "iopub.status.idle": "2021-06-15T13:48:30.323032Z",
          "shell.execute_reply.started": "2021-06-15T13:48:28.772361Z",
          "shell.execute_reply": "2021-06-15T13:48:30.322166Z"
        },
        "trusted": true,
        "id": "J3L6_mzrHlg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the Distribution of text length for Negative sentiment tweets."
      ],
      "metadata": {
        "_uuid": "7de8e7ce-1b0b-49be-a353-f2f31fd6d863",
        "_cell_guid": "2bd343a0-b9df-49f2-9f02-bfb2789d4491",
        "trusted": true,
        "id": "HIXgy-p_Hlg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(14,7))\n",
        "df['length'] = df.clean_text.str.split().apply(len)\n",
        "ax1 = fig.add_subplot(122)\n",
        "sns.histplot(df[df['category']=='Negative']['length'], ax=ax1,color='red')\n",
        "describe = df.length[df.category=='Negative'].describe().to_frame().round(2)\n",
        "\n",
        "ax2 = fig.add_subplot(121)\n",
        "ax2.axis('off')\n",
        "font_size = 14\n",
        "bbox = [0, 0, 1, 1]\n",
        "table = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\n",
        "table.set_fontsize(font_size)\n",
        "fig.suptitle('Distribution of text length for Negative sentiment tweets.', fontsize=16)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "_uuid": "b63bbb0c-78ea-4a18-a94e-52946af6ca3e",
        "_cell_guid": "7e2ae773-c18b-4d00-a56a-4519b858ed51",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:48:30.324267Z",
          "iopub.execute_input": "2021-06-15T13:48:30.324749Z",
          "iopub.status.idle": "2021-06-15T13:48:31.707317Z",
          "shell.execute_reply.started": "2021-06-15T13:48:30.324711Z",
          "shell.execute_reply": "2021-06-15T13:48:31.706317Z"
        },
        "trusted": true,
        "id": "Mc8W0ngmHlg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the Pie chart of the percentage of different sentiments of all the tweets"
      ],
      "metadata": {
        "_uuid": "bbb313ab-aca3-4823-94b9-235a9bab845e",
        "_cell_guid": "36f7bdd4-1a5d-4575-9481-e5d317f50049",
        "trusted": true,
        "id": "eYFJ-uhdHlg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "fig = px.pie(df, names='category', title ='Pie chart of different sentiments of tweets')\n",
        "fig.show()"
      ],
      "metadata": {
        "_uuid": "3ec20e3c-4a70-4ec2-b694-e13286d1c4e8",
        "_cell_guid": "98207def-3cc9-4f73-8726-b1f71048c3b6",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:48:31.708555Z",
          "iopub.execute_input": "2021-06-15T13:48:31.709046Z",
          "iopub.status.idle": "2021-06-15T13:48:32.627791Z",
          "shell.execute_reply.started": "2021-06-15T13:48:31.709008Z",
          "shell.execute_reply": "2021-06-15T13:48:32.626969Z"
        },
        "trusted": true,
        "id": "IamgLSfIHlg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['length'], axis=1, inplace=True)\n",
        "df.head"
      ],
      "metadata": {
        "_uuid": "03aef98a-4837-4fcb-b117-085a421a35dc",
        "_cell_guid": "d01c95a4-3f68-437d-9063-4753f1258c84",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:48:32.628933Z",
          "iopub.execute_input": "2021-06-15T13:48:32.629353Z",
          "iopub.status.idle": "2021-06-15T13:48:32.647799Z",
          "shell.execute_reply.started": "2021-06-15T13:48:32.629321Z",
          "shell.execute_reply": "2021-06-15T13:48:32.647007Z"
        },
        "trusted": true,
        "id": "YYEu6HcfHlg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Visualizing data into wordclouds\n",
        "\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "def wordcount_gen(df, category):\n",
        "    '''\n",
        "    Generating Word Cloud\n",
        "    inputs:\n",
        "       - df: tweets dataset\n",
        "       - category: Positive/Negative/Neutral\n",
        "    '''\n",
        "    # Combine all tweets\n",
        "    combined_tweets = \" \".join([tweet for tweet in df[df.category==category]['clean_text']])\n",
        "\n",
        "    # Initialize wordcloud object\n",
        "    wc = WordCloud(background_color='white',\n",
        "                   max_words=50,\n",
        "                   stopwords = STOPWORDS)\n",
        "\n",
        "    # Generate and plot wordcloud\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(wc.generate(combined_tweets))\n",
        "    plt.title('{} Sentiment Words'.format(category), fontsize=20)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Positive tweet words\n",
        "wordcount_gen(df, 'Positive')\n",
        "\n",
        "# Negative tweet words\n",
        "wordcount_gen(df, 'Negative')\n",
        "\n",
        "# Neutral tweet words\n",
        "wordcount_gen(df, 'Neutral')\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "3fe90f83-0774-4e1e-b08b-817816bf93d7",
        "_cell_guid": "21a5ed8a-b496-4b90-be85-ea13c8224da7",
        "execution": {
          "iopub.status.busy": "2021-06-15T13:48:32.649007Z",
          "iopub.execute_input": "2021-06-15T13:48:32.649487Z",
          "iopub.status.idle": "2021-06-15T13:48:51.532388Z",
          "shell.execute_reply.started": "2021-06-15T13:48:32.649449Z",
          "shell.execute_reply": "2021-06-15T13:48:51.53169Z"
        },
        "trusted": true,
        "id": "2OUgprNTHlg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "_uuid": "f03fa6a3-85de-4c49-8e0f-eebc3abb0f74",
        "_cell_guid": "de0f402f-09e1-4bc6-bb6a-afc89ee55ad0",
        "trusted": true,
        "id": "GmKfsl-QHlg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tweet_to_words(tweet):\n",
        "    ''' Convert tweet text into a sequence of words '''\n",
        "\n",
        "    # convert to lowercase\n",
        "    text = tweet.lower()\n",
        "    # remove non letters\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
        "    # tokenize\n",
        "    words = text.split()\n",
        "    # remove stopwords\n",
        "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
        "    # apply stemming\n",
        "    words = [PorterStemmer().stem(w) for w in words]\n",
        "    # return list\n",
        "    return words\n",
        "\n",
        "print(\"\\nOriginal tweet ->\", df['clean_text'][0])\n",
        "print(\"\\nProcessed tweet ->\", tweet_to_words(df['clean_text'][0]))"
      ],
      "metadata": {
        "_uuid": "b452e7a7-8132-4951-a932-d007b3955870",
        "_cell_guid": "13bf9b06-9a96-4e39-b3dd-8c24d848c2c2",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:48:51.533574Z",
          "iopub.execute_input": "2021-06-15T13:48:51.533906Z",
          "iopub.status.idle": "2021-06-15T13:48:51.553512Z",
          "shell.execute_reply.started": "2021-06-15T13:48:51.533869Z",
          "shell.execute_reply": "2021-06-15T13:48:51.552693Z"
        },
        "trusted": true,
        "id": "K4RgqRJfHlg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply data processing to each tweet\n",
        "X = list(map(tweet_to_words, df['clean_text']))"
      ],
      "metadata": {
        "_uuid": "792352af-397b-42b1-bd22-97a83548c929",
        "_cell_guid": "fdd4e291-fc5e-436a-bae3-fdde3fad279f",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:48:51.555058Z",
          "iopub.execute_input": "2021-06-15T13:48:51.555414Z",
          "iopub.status.idle": "2021-06-15T13:56:56.429724Z",
          "shell.execute_reply.started": "2021-06-15T13:48:51.555381Z",
          "shell.execute_reply": "2021-06-15T13:56:56.428932Z"
        },
        "trusted": true,
        "id": "pFG52YRJHlg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode target labels\n",
        "le = LabelEncoder()\n",
        "Y = le.fit_transform(df['category'])"
      ],
      "metadata": {
        "_uuid": "afe4880d-4718-4ac4-a475-4ff4ca592ae7",
        "_cell_guid": "4f06bc18-d12c-4c77-90d3-97e056f2336e",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:56:56.431296Z",
          "iopub.execute_input": "2021-06-15T13:56:56.431632Z",
          "iopub.status.idle": "2021-06-15T13:56:56.489453Z",
          "shell.execute_reply.started": "2021-06-15T13:56:56.431598Z",
          "shell.execute_reply": "2021-06-15T13:56:56.488825Z"
        },
        "trusted": true,
        "id": "AJA-NrVxHlg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[0])\n",
        "print(Y[0])"
      ],
      "metadata": {
        "_uuid": "84bb5739-621a-4897-9ba1-07d5f784e35d",
        "_cell_guid": "c19810ad-086e-4304-b3d4-47c2135e17cc",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:56:56.490583Z",
          "iopub.execute_input": "2021-06-15T13:56:56.490898Z",
          "iopub.status.idle": "2021-06-15T13:56:56.496022Z",
          "shell.execute_reply.started": "2021-06-15T13:56:56.490859Z",
          "shell.execute_reply": "2021-06-15T13:56:56.494153Z"
        },
        "trusted": true,
        "id": "EuuiStBrHlg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and test split"
      ],
      "metadata": {
        "_uuid": "a69f5a61-e641-4d50-be8a-561ffdd13aeb",
        "_cell_guid": "f9ff9b12-980d-4a0c-b205-7abba133ba3e",
        "trusted": true,
        "id": "2mfFZVQOHlg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = pd.get_dummies(df['category'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)"
      ],
      "metadata": {
        "_uuid": "16792e3e-f18b-40aa-968a-f8d72ec01908",
        "_cell_guid": "246d6ef8-7ed1-404f-9a55-2b74c407418c",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:56:56.497714Z",
          "iopub.execute_input": "2021-06-15T13:56:56.49807Z",
          "iopub.status.idle": "2021-06-15T13:56:56.734094Z",
          "shell.execute_reply.started": "2021-06-15T13:56:56.498037Z",
          "shell.execute_reply": "2021-06-15T13:56:56.733037Z"
        },
        "trusted": true,
        "id": "XX7Vh6iiHlg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of words (BOW) feature extraction"
      ],
      "metadata": {
        "_uuid": "59bb8dab-377f-4c0f-b0e7-dc4977fb7817",
        "_cell_guid": "ac8e697c-c61f-4de8-b80e-2a014c6e4e41",
        "trusted": true,
        "id": "LHoKQum5Hlg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vocabulary_size = 5000\n",
        "\n",
        "# Tweets have already been preprocessed hence dummy function will be passed in\n",
        "# to preprocessor & tokenizer step\n",
        "count_vector = CountVectorizer(max_features=vocabulary_size,\n",
        "#                               ngram_range=(1,2),    # unigram and bigram\n",
        "                                preprocessor=lambda x: x,\n",
        "                               tokenizer=lambda x: x)\n",
        "#tfidf_vector = TfidfVectorizer(lowercase=True, stop_words='english')\n",
        "\n",
        "# Fit the training data\n",
        "X_train = count_vector.fit_transform(X_train).toarray()\n",
        "\n",
        "# Transform testing data\n",
        "X_test = count_vector.transform(X_test).toarray()"
      ],
      "metadata": {
        "_uuid": "3eb577ca-80df-496b-91e1-ba0aab21fe11",
        "_cell_guid": "c9e9e40f-3610-4d2f-87d3-a124082b688a",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:56:56.735579Z",
          "iopub.execute_input": "2021-06-15T13:56:56.736192Z",
          "iopub.status.idle": "2021-06-15T13:57:00.775493Z",
          "shell.execute_reply.started": "2021-06-15T13:56:56.736152Z",
          "shell.execute_reply": "2021-06-15T13:57:00.774712Z"
        },
        "trusted": true,
        "id": "A6qGsGmVHlg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import sklearn.preprocessing as pr\n",
        "\n",
        "# Normalize BoW features in training and test set\n",
        "#X_train = pr.normalize(X_train, axis=1)\n",
        "#X_test  = pr.normalize(X_test, axis=1)"
      ],
      "metadata": {
        "_uuid": "8577fe0b-162e-42c0-8d58-c377d77e462f",
        "_cell_guid": "172f7f81-9e69-4b17-838b-23a2f2206d45",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:57:00.776727Z",
          "iopub.execute_input": "2021-06-15T13:57:00.777071Z",
          "iopub.status.idle": "2021-06-15T13:57:00.78071Z",
          "shell.execute_reply.started": "2021-06-15T13:57:00.777038Z",
          "shell.execute_reply": "2021-06-15T13:57:00.779947Z"
        },
        "trusted": true,
        "id": "_CnTraOAHlg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print first 200 words/tokens\n",
        "print(count_vector.get_feature_names()[0:200])"
      ],
      "metadata": {
        "_uuid": "832389ad-89a0-4302-bfde-62fbe5b767e5",
        "_cell_guid": "87b2a010-f51b-4c8e-bfc7-7db8b9190a44",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:57:00.78202Z",
          "iopub.execute_input": "2021-06-15T13:57:00.7825Z",
          "iopub.status.idle": "2021-06-15T13:57:00.800575Z",
          "shell.execute_reply.started": "2021-06-15T13:57:00.782465Z",
          "shell.execute_reply": "2021-06-15T13:57:00.799703Z"
        },
        "trusted": true,
        "id": "V85qd2kVHlg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the BoW feature vector\n",
        "plt.plot(X_train[2,:])\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "_uuid": "67d7be8f-57ce-4f5d-9684-62ace517a549",
        "_cell_guid": "3357d08e-29cd-4cfb-99f1-edbef089445a",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:57:00.80208Z",
          "iopub.execute_input": "2021-06-15T13:57:00.802484Z",
          "iopub.status.idle": "2021-06-15T13:57:00.920572Z",
          "shell.execute_reply.started": "2021-06-15T13:57:00.802448Z",
          "shell.execute_reply": "2021-06-15T13:57:00.919626Z"
        },
        "trusted": true,
        "id": "0I5RJBFMHlg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing & Padding"
      ],
      "metadata": {
        "_uuid": "e0790578-2f97-45d2-ac1b-eca2dac01166",
        "_cell_guid": "fa0fa00f-3b2e-4bf3-9e56-a3c0e99a5033",
        "trusted": true,
        "id": "F7TduZdQHlg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_words = 5000\n",
        "max_len=50\n",
        "\n",
        "def tokenize_pad_sequences(text):\n",
        "    '''\n",
        "    This function tokenize the input text into sequnences of intergers and then\n",
        "    pad each sequence to the same length\n",
        "    '''\n",
        "    # Text tokenization\n",
        "    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    # Transforms text to a sequence of integers\n",
        "    X = tokenizer.texts_to_sequences(text)\n",
        "    # Pad sequences to the same length\n",
        "    X = pad_sequences(X, padding='post', maxlen=max_len)\n",
        "    # return sequences\n",
        "    return X, tokenizer\n",
        "\n",
        "print('Before Tokenization & Padding \\n', df['clean_text'][0])\n",
        "X, tokenizer = tokenize_pad_sequences(df['clean_text'])\n",
        "print('After Tokenization & Padding \\n', X[0])"
      ],
      "metadata": {
        "_uuid": "09fe6021-b3ac-419c-ab3b-3a1f76379a2c",
        "_cell_guid": "ee7d87c0-f428-47d0-92cf-4e3b157d83eb",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:57:00.9221Z",
          "iopub.execute_input": "2021-06-15T13:57:00.922444Z",
          "iopub.status.idle": "2021-06-15T13:57:10.389152Z",
          "shell.execute_reply.started": "2021-06-15T13:57:00.92241Z",
          "shell.execute_reply": "2021-06-15T13:57:10.387288Z"
        },
        "trusted": true,
        "id": "Jbv553yVHlg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving tokenized data"
      ],
      "metadata": {
        "_uuid": "3428c844-00ef-4add-b27c-794d7aeb5c9b",
        "_cell_guid": "918b178c-362d-4307-950c-67f205eadc68",
        "trusted": true,
        "id": "QRheGVfgHlg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# saving\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# loading\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ],
      "metadata": {
        "_uuid": "0dd5da2a-01b2-4697-a201-736d6a9bd9f1",
        "_cell_guid": "53611a76-f3fe-4a8b-a86e-a956ad8dc2b3",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:57:10.390514Z",
          "iopub.execute_input": "2021-06-15T13:57:10.390829Z",
          "iopub.status.idle": "2021-06-15T13:57:10.662767Z",
          "shell.execute_reply.started": "2021-06-15T13:57:10.390795Z",
          "shell.execute_reply": "2021-06-15T13:57:10.661979Z"
        },
        "trusted": true,
        "id": "QGR-VT1SHlg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train & Test Split"
      ],
      "metadata": {
        "_uuid": "761a0192-6c66-44b1-988f-530dfc4ac8ce",
        "_cell_guid": "17d33b73-9e80-45f3-8dc2-d33353ac7fc7",
        "trusted": true,
        "id": "O1czi78bHlg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = pd.get_dummies(df['category'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
        "print('Train Set ->', X_train.shape, y_train.shape)\n",
        "print('Validation Set ->', X_val.shape, y_val.shape)\n",
        "print('Test Set ->', X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "_uuid": "8218591d-f858-4e1f-a877-10e74f0885f3",
        "_cell_guid": "ab4d79f8-27f4-4384-866a-280442941ef7",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:57:10.664774Z",
          "iopub.execute_input": "2021-06-15T13:57:10.66547Z",
          "iopub.status.idle": "2021-06-15T13:57:10.905583Z",
          "shell.execute_reply.started": "2021-06-15T13:57:10.665432Z",
          "shell.execute_reply": "2021-06-15T13:57:10.9044Z"
        },
        "trusted": true,
        "id": "tW2Bv2Q7Hlg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def f1_score(precision, recall):\n",
        "    ''' Function to calculate f1 score '''\n",
        "\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "metadata": {
        "_uuid": "73f3276a-0531-4142-8695-3af993665794",
        "_cell_guid": "68b2697c-f73a-4f55-9dfa-f47319a60d3e",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:57:10.907553Z",
          "iopub.execute_input": "2021-06-15T13:57:10.907915Z",
          "iopub.status.idle": "2021-06-15T13:57:10.913098Z",
          "shell.execute_reply.started": "2021-06-15T13:57:10.907864Z",
          "shell.execute_reply": "2021-06-15T13:57:10.912086Z"
        },
        "trusted": true,
        "id": "nbS-EPhJHlg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bidirectional LSTM Using NN"
      ],
      "metadata": {
        "_uuid": "f4466e64-4f6a-4868-b430-153d7511a83a",
        "_cell_guid": "819dd6c6-7269-4ebd-b130-089311a92c26",
        "trusted": true,
        "id": "UrNGzLVBHlg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n",
        "from keras.metrics import Precision, Recall\n",
        "from keras.optimizers import SGD\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import datasets\n",
        "\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import History\n",
        "\n",
        "from keras import losses\n",
        "\n",
        "vocab_size = 5000\n",
        "embedding_size = 32\n",
        "epochs=20\n",
        "learning_rate = 0.1\n",
        "decay_rate = learning_rate / epochs\n",
        "momentum = 0.8\n",
        "\n",
        "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
        "# Build model\n",
        "model= Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_size, input_length=max_len))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(3, activation='softmax'))"
      ],
      "metadata": {
        "_uuid": "bb98be32-1c7a-442a-bd28-7bf3a01b6050",
        "_cell_guid": "34db76fb-fc02-4aa9-86ef-88d30cb5aa01",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:57:10.919179Z",
          "iopub.execute_input": "2021-06-15T13:57:10.919564Z",
          "iopub.status.idle": "2021-06-15T13:57:13.675214Z",
          "shell.execute_reply.started": "2021-06-15T13:57:10.919531Z",
          "shell.execute_reply": "2021-06-15T13:57:13.674327Z"
        },
        "trusted": true,
        "id": "qh5mnAO2Hlg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "_uuid": "7f4f6d96-8335-456d-b3d2-baaceec72fb2",
        "_cell_guid": "7ffc99fa-11d6-409d-af3c-c4c77add6fe7",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:57:13.676584Z",
          "iopub.execute_input": "2021-06-15T13:57:13.676925Z",
          "iopub.status.idle": "2021-06-15T13:57:14.163667Z",
          "shell.execute_reply.started": "2021-06-15T13:57:13.676874Z",
          "shell.execute_reply": "2021-06-15T13:57:14.162848Z"
        },
        "trusted": true,
        "id": "_Zh3j3T0Hlg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd,\n",
        "               metrics=['accuracy', Precision(), Recall()])\n",
        "\n",
        "# Train model\n",
        "\n",
        "batch_size = 64\n",
        "history = model.fit(X_train, y_train,\n",
        "                      validation_data=(X_val, y_val),\n",
        "                      batch_size=batch_size, epochs=epochs, verbose=1)"
      ],
      "metadata": {
        "_uuid": "1f2778a3-1caf-4927-aef5-bdef1048457c",
        "_cell_guid": "402d6b63-3bf3-48cf-b590-163908ae68ae",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T13:57:14.16538Z",
          "iopub.execute_input": "2021-06-15T13:57:14.165737Z",
          "iopub.status.idle": "2021-06-15T14:03:03.293306Z",
          "shell.execute_reply.started": "2021-06-15T13:57:14.165698Z",
          "shell.execute_reply": "2021-06-15T14:03:03.2925Z"
        },
        "trusted": true,
        "id": "N4yk1sZFHlg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Accuracy & Loss"
      ],
      "metadata": {
        "_uuid": "844d703e-882a-47c6-a3ba-b4622aa98626",
        "_cell_guid": "f657ab51-e63e-47b3-8995-f13d79c570cd",
        "trusted": true,
        "id": "Bqg705eHHlg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on the test set\n",
        "loss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=0)\n",
        "# Print metrics\n",
        "print('')\n",
        "print('Accuracy  : {:.4f}'.format(accuracy))\n",
        "print('Precision : {:.4f}'.format(precision))\n",
        "print('Recall    : {:.4f}'.format(recall))\n",
        "print('F1 Score  : {:.4f}'.format(f1_score(precision, recall)))"
      ],
      "metadata": {
        "_uuid": "42e7ee95-8192-4410-9c47-9f950a7a870e",
        "_cell_guid": "3310bf9e-95ff-4d89-8e64-0f677054bddd",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T14:03:03.296693Z",
          "iopub.execute_input": "2021-06-15T14:03:03.296973Z",
          "iopub.status.idle": "2021-06-15T14:03:07.916302Z",
          "shell.execute_reply.started": "2021-06-15T14:03:03.296947Z",
          "shell.execute_reply": "2021-06-15T14:03:07.915513Z"
        },
        "trusted": true,
        "id": "8taG9IGrHlg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_hist(history):\n",
        "    '''Function to plot history for accuracy and loss'''\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
        "    # first plot\n",
        "    ax[0].plot(history.history['accuracy'])\n",
        "    ax[0].plot(history.history['val_accuracy'])\n",
        "    ax[0].set_title('Model Accuracy')\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    ax[0].set_ylabel('accuracy')\n",
        "    ax[0].legend(['train', 'validation'], loc='best')\n",
        "    # second plot\n",
        "    ax[1].plot(history.history['loss'])\n",
        "    ax[1].plot(history.history['val_loss'])\n",
        "    ax[1].set_title('Model Loss')\n",
        "    ax[1].set_xlabel('epoch')\n",
        "    ax[1].set_ylabel('loss')\n",
        "    ax[1].legend(['train', 'validation'], loc='best')\n",
        "\n",
        "plot_training_hist(history)"
      ],
      "metadata": {
        "_uuid": "46151ee3-5a8b-44fe-b3fb-36c737323efb",
        "_cell_guid": "ab83ad9c-f59b-4209-9a0e-6e2d6535290e",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T14:03:07.919134Z",
          "iopub.execute_input": "2021-06-15T14:03:07.919393Z",
          "iopub.status.idle": "2021-06-15T14:03:08.186032Z",
          "shell.execute_reply.started": "2021-06-15T14:03:07.919368Z",
          "shell.execute_reply": "2021-06-15T14:03:08.185301Z"
        },
        "trusted": true,
        "id": "KEptEOaVHlg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Confusion Matrix"
      ],
      "metadata": {
        "_uuid": "21d1d657-02fc-4db4-8ea1-fee1327b76c1",
        "_cell_guid": "45f9ed85-ee6e-4012-b40d-33455fdb8474",
        "trusted": true,
        "id": "f4JB2GSQHlhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(model, X_test, y_test):\n",
        "    '''Function to plot confusion matrix for the passed model and the data'''\n",
        "\n",
        "    sentiment_classes = ['Negative', 'Neutral', 'Positive']\n",
        "    # use model to do the prediction\n",
        "    y_pred = model.predict(X_test)\n",
        "    # compute confusion matrix\n",
        "    cm = confusion_matrix(np.argmax(np.array(y_test),axis=1), np.argmax(y_pred, axis=1))\n",
        "    # plot confusion matrix\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, cmap=plt.cm.Blues, annot=True, fmt='d',\n",
        "                xticklabels=sentiment_classes,\n",
        "                yticklabels=sentiment_classes)\n",
        "    plt.title('Confusion matrix', fontsize=16)\n",
        "    plt.xlabel('Actual label', fontsize=12)\n",
        "    plt.ylabel('Predicted label', fontsize=12)\n",
        "\n",
        "plot_confusion_matrix(model, X_test, y_test)"
      ],
      "metadata": {
        "_uuid": "4fde18b3-a28c-44e2-8815-09efb6859b54",
        "_cell_guid": "73d977c3-21eb-410d-8f64-91eb6c758e6e",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T14:03:08.189031Z",
          "iopub.execute_input": "2021-06-15T14:03:08.189285Z",
          "iopub.status.idle": "2021-06-15T14:03:11.87777Z",
          "shell.execute_reply.started": "2021-06-15T14:03:08.189259Z",
          "shell.execute_reply": "2021-06-15T14:03:11.877031Z"
        },
        "trusted": true,
        "id": "Vq5rxcrEHlhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model save and load for the prediction"
      ],
      "metadata": {
        "_uuid": "f2225ada-4e4c-4b47-82b2-0b43b28dfb40",
        "_cell_guid": "2aa7fd7a-c5e5-430a-8f4a-79c331e2486e",
        "trusted": true,
        "id": "wcF847DiHlhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model architecture & the weights\n",
        "model.save('best_model.h5')\n",
        "print('Best model saved')"
      ],
      "metadata": {
        "_uuid": "7005da8f-1988-4e7b-a34d-f1355fe504f7",
        "_cell_guid": "567c602f-0579-4257-8f6b-7b6039fa6729",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T14:03:11.880629Z",
          "iopub.execute_input": "2021-06-15T14:03:11.880904Z",
          "iopub.status.idle": "2021-06-15T14:03:11.916253Z",
          "shell.execute_reply.started": "2021-06-15T14:03:11.880862Z",
          "shell.execute_reply": "2021-06-15T14:03:11.915216Z"
        },
        "trusted": true,
        "id": "Ln9M5mf6HlhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Load model\n",
        "model = load_model('best_model.h5')\n",
        "\n",
        "def predict_class(text):\n",
        "    '''Function to predict sentiment class of the passed text'''\n",
        "\n",
        "    sentiment_classes = ['Negative', 'Neutral', 'Positive']\n",
        "    max_len=50\n",
        "\n",
        "    # Transforms text to a sequence of integers using a tokenizer object\n",
        "    xt = tokenizer.texts_to_sequences(text)\n",
        "    # Pad sequences to the same length\n",
        "    xt = pad_sequences(xt, padding='post', maxlen=max_len)\n",
        "    # Do the prediction using the loaded model\n",
        "    yt = model.predict(xt).argmax(axis=1)\n",
        "    # Print the predicted sentiment\n",
        "    print('The predicted sentiment is', sentiment_classes[yt[0]])"
      ],
      "metadata": {
        "_uuid": "bcfae6c2-a987-443b-a3d4-46722dca3827",
        "_cell_guid": "80ff55f2-b918-4dea-84a6-d0260c65a0fa",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T14:03:11.917567Z",
          "iopub.execute_input": "2021-06-15T14:03:11.917916Z",
          "iopub.status.idle": "2021-06-15T14:03:12.400898Z",
          "shell.execute_reply.started": "2021-06-15T14:03:11.917862Z",
          "shell.execute_reply": "2021-06-15T14:03:12.400148Z"
        },
        "trusted": true,
        "id": "yTFwalFLHlhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_class(['\"I hate when I have to call and wake people up'])"
      ],
      "metadata": {
        "_uuid": "6975be81-c751-4741-903f-858470c72ce0",
        "_cell_guid": "886580de-a4b7-4d6c-9b44-f18541be7761",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T14:03:12.402753Z",
          "iopub.execute_input": "2021-06-15T14:03:12.403372Z",
          "iopub.status.idle": "2021-06-15T14:03:13.294193Z",
          "shell.execute_reply.started": "2021-06-15T14:03:12.403332Z",
          "shell.execute_reply": "2021-06-15T14:03:13.293434Z"
        },
        "trusted": true,
        "id": "DuVkgrsyHlhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_class(['The food was meh'])"
      ],
      "metadata": {
        "_uuid": "efe10049-9dd3-45f4-b175-517f1da3e5bb",
        "_cell_guid": "bf0d2be3-1ee1-4202-b6cf-c943db11c215",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T14:03:13.296105Z",
          "iopub.execute_input": "2021-06-15T14:03:13.296361Z",
          "iopub.status.idle": "2021-06-15T14:03:13.337723Z",
          "shell.execute_reply.started": "2021-06-15T14:03:13.296336Z",
          "shell.execute_reply": "2021-06-15T14:03:13.337075Z"
        },
        "trusted": true,
        "id": "NN6-4UxuHlhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_class(['He is a best minister india ever had seen'])"
      ],
      "metadata": {
        "_uuid": "9f778494-05db-4484-ab8e-f9821c9d85bd",
        "_cell_guid": "80949345-470c-4a79-b866-cf55a56b7375",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T14:03:13.340408Z",
          "iopub.execute_input": "2021-06-15T14:03:13.340652Z",
          "iopub.status.idle": "2021-06-15T14:03:13.379267Z",
          "shell.execute_reply.started": "2021-06-15T14:03:13.340628Z",
          "shell.execute_reply": "2021-06-15T14:03:13.378289Z"
        },
        "trusted": true,
        "id": "faeYGDkdHlhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the 'tweepy' API, you need to create an account with Twitter Developer. After creating the account, go to 'Get Started' option and navigate to the 'Create an app' option. After you create the app, note down the below required credentials from there."
      ],
      "metadata": {
        "_uuid": "aab55f67-aaf6-46a1-99f6-7c618d54c9c7",
        "_cell_guid": "263301a3-2a40-4bd3-abf2-1f9f252c15f4",
        "trusted": true,
        "id": "wb7R1U-cHlhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetching data from twitter\n",
        "\n",
        "To get started, you’ll need to do the following things:\n",
        "\n",
        "  * Set up a Twitter account if you don’t have one already.\n",
        "  * Using your Twitter account, you will need to apply for Developer Access and then create an application that will generate the API credentials that you will use to access Twitter from Python.\n",
        "  *  Import the `tweepy` package."
      ],
      "metadata": {
        "_uuid": "6d78ce42-70f5-4a3d-89af-87d320a71e9f",
        "_cell_guid": "e6cdf550-bd68-4967-8732-e54293237a19",
        "trusted": true,
        "id": "suu5iY7cHlhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authorization"
      ],
      "metadata": {
        "_uuid": "ae4ecfef-9342-45fb-98a3-b79672baa3d1",
        "_cell_guid": "d2f08a09-60dc-4ee8-aab1-04e23487ecd5",
        "trusted": true,
        "id": "3L6GJFaiHlhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After installing and importing `tweepy` package (already done in the beginning),to access the Twitter API, you will need 4 things from the your Twitter App page. These keys are located in your Twitter app settings in the Keys and Access Tokens tab.\n",
        "\n",
        "    consumer key\n",
        "    consumer seceret key\n",
        "    access token key\n",
        "    access token secret key\n",
        "\n",
        "Do not share these with anyone else because these values are specific to your app."
      ],
      "metadata": {
        "_uuid": "2eee417f-0a18-476e-b0bc-f87d5ca87053",
        "_cell_guid": "7c3a3792-3c24-4b97-85be-a27f601de2af",
        "trusted": true,
        "id": "LpF28LXtHlhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting authorization\n",
        "consumer_key= 'yourkeyhere'\n",
        "consumer_secret= 'yourkeyhere'\n",
        "access_token= 'yourkeyhere'\n",
        "access_token_secret= 'yourkeyhere'\n",
        "\n",
        "auth = tw.OAuthHandler(consumer_key, consumer_key_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tw.API(auth, wait_on_rate_limit=True)"
      ],
      "metadata": {
        "_uuid": "b1650044-17e3-4604-83e1-77ccce4854c0",
        "_cell_guid": "1a181a79-ae1a-43eb-9296-b7bafeaa503c",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T14:03:13.380688Z",
          "iopub.execute_input": "2021-06-15T14:03:13.381039Z",
          "iopub.status.idle": "2021-06-15T14:03:13.387216Z",
          "shell.execute_reply.started": "2021-06-15T14:03:13.381005Z",
          "shell.execute_reply": "2021-06-15T14:03:13.385212Z"
        },
        "trusted": true,
        "id": "WqQqR9UAHlhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search Twitter for Tweets"
      ],
      "metadata": {
        "_uuid": "572ab94f-9be7-4c76-97c1-0447257c96a6",
        "_cell_guid": "6dc6e135-aacb-4036-bc36-0e58ed88ca05",
        "trusted": true,
        "id": "y9qQmUecHlhB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Searching for a hashtag from a specified date\n",
        "\n",
        "Now you are ready to search Twitter for recent tweets! You will use the `.Cursor` method to get an object containing tweets containing the hashtag #wildfires.\n",
        "\n",
        "To create this query, you will define the:\n",
        "\n",
        "   * Search term - in this case #WorkFromHome\n",
        "   * the start date of your search"
      ],
      "metadata": {
        "_uuid": "dc835d2a-9994-4c72-98eb-fa6417c10c3f",
        "_cell_guid": "7972ab3d-44a9-4474-b91a-2d56506e3859",
        "trusted": true,
        "id": "8Rs5B7izHlhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the search term and the date_since date as variables\n",
        "search_words = \"#WorkFromHome\"\n",
        "date_since = \"2021-06-10\""
      ],
      "metadata": {
        "_uuid": "10e880c3-2815-44e8-b9da-a7f4bf211988",
        "_cell_guid": "940aad4f-619e-417c-b628-46adfe954d9a",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T14:03:13.388646Z",
          "iopub.execute_input": "2021-06-15T14:03:13.389186Z",
          "iopub.status.idle": "2021-06-15T14:03:13.399131Z",
          "shell.execute_reply.started": "2021-06-15T14:03:13.38915Z",
          "shell.execute_reply": "2021-06-15T14:03:13.398363Z"
        },
        "trusted": true,
        "id": "6EjcJUy4HlhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can restrict the number of tweets returned by specifying a number in the `.items()` method. `.items(10)` will return 10 of the most recent tweets."
      ],
      "metadata": {
        "_uuid": "a0f7a434-52e5-452f-bd1b-9e76e6e02dc2",
        "_cell_guid": "e329e214-fecd-4cb1-a713-03269d06d76d",
        "trusted": true,
        "id": "boObQkJ7HlhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect tweets\n",
        "tweets = tw.Cursor(api.search,\n",
        "              q=search_words,\n",
        "              lang=\"en\",\n",
        "              since=date_since).items(10)\n",
        "\n",
        "# Iterate and print tweets\n",
        "for tweet in tweets:\n",
        "    print(tweet.text)\n",
        "    predict_class([tweet.text]) #predicting sentiment\n",
        "    print(\"\")"
      ],
      "metadata": {
        "_uuid": "435c7fd0-140a-4cfe-9453-3552a8109c1a",
        "_cell_guid": "3ebaeff9-3d35-43ec-836a-0e28e1f4d7da",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T14:03:13.400479Z",
          "iopub.execute_input": "2021-06-15T14:03:13.400913Z",
          "iopub.status.idle": "2021-06-15T14:03:14.37241Z",
          "shell.execute_reply.started": "2021-06-15T14:03:13.400867Z",
          "shell.execute_reply": "2021-06-15T14:03:14.37107Z"
        },
        "trusted": true,
        "id": "w1bBy_rtHlhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Searching for a keyword\n",
        "\n",
        "You can pass the keyword of your interest here and the number of tweets (in this case 10) to be downloaded through the tweepy API."
      ],
      "metadata": {
        "_uuid": "d374a994-678d-4aee-a62f-14ad0a227969",
        "_cell_guid": "64698d02-019a-404f-af13-77a27dbe33a5",
        "trusted": true,
        "id": "HJofmhVNHlhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet in api.search(q=\"elonmusk\", lang=\"en\", rpp=10):\n",
        "    print(f\"{tweet.user.name}:{tweet.text}\")\n",
        "    predict_class([tweet.text]) #predicting sentiment\n",
        "    print(\"\")"
      ],
      "metadata": {
        "_uuid": "a6009af5-08b4-4e91-944a-5cf47ece9ae8",
        "_cell_guid": "669f7b54-985b-4e1b-addc-b9f743d08b5c",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T14:03:14.374389Z",
          "iopub.execute_input": "2021-06-15T14:03:14.374733Z",
          "iopub.status.idle": "2021-06-15T14:03:15.501874Z",
          "shell.execute_reply.started": "2021-06-15T14:03:14.374694Z",
          "shell.execute_reply": "2021-06-15T14:03:15.500512Z"
        },
        "trusted": true,
        "id": "t5Fa6TTXHlhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Searching for a username\n",
        "\n",
        "The `user_timeline()` method of the API class in Tweepy module is used to get the most recent statuses posted from the authenticating user or the user specified.\n",
        "\n",
        "* `screen_name` = user id name of the twitter user\n",
        "* `count` = no. of tweets to be fetched\n",
        "* `lang` = language of tweet\n",
        "* `tweet_mode = 'extended'` swaps the text index for full_text, and prevents a primary tweet longer than 140 characters from being truncated.\n",
        "\n",
        "\n",
        "The below query pulls 10 tweets from Twitter user @ of english language and predicts its sentiments."
      ],
      "metadata": {
        "_uuid": "79c0086c-a59f-4fa5-a3ad-0de380445c2a",
        "_cell_guid": "d22e9433-b51d-4a95-936b-e595eeda787a",
        "trusted": true,
        "id": "Uct6Fgj6HlhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "posts = api.user_timeline(screen_name=\"elonmusk\", count = 10, lang =\"en\", tweet_mode=\"extended\")\n",
        "\n",
        "l=[]\n",
        "i=1\n",
        "\n",
        "for tweet in posts[:10]:\n",
        "    l.append(tweet.full_text)\n",
        "    i=i+1\n",
        "for j in range (0,9):\n",
        "    print(l[j])\n",
        "    predict_class([l[j]])  #predicting sentiment\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "_uuid": "9f2529d0-7ad4-476c-a17d-95d2ea3af581",
        "_cell_guid": "7a53d142-1b6b-45bf-b5a9-a34440ab5c95",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2021-06-15T14:03:15.503291Z",
          "iopub.execute_input": "2021-06-15T14:03:15.503618Z",
          "iopub.status.idle": "2021-06-15T14:03:16.197405Z",
          "shell.execute_reply.started": "2021-06-15T14:03:15.503581Z",
          "shell.execute_reply": "2021-06-15T14:03:16.196622Z"
        },
        "trusted": true,
        "id": "pqbE0jQaHlhB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}